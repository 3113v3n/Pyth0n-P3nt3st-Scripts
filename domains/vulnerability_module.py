from typing import List, Dict, Optional
from handlers import FileHandler, DisplayHandler
from utils import Config, FilterVulnerabilities
from functools import reduce, partial
from concurrent.futures import ThreadPoolExecutor
import pandas as pd


class VulnerabilityAnalysis(
    FileHandler,
    Config,
    FilterVulnerabilities,
    DisplayHandler
):
    """Class that handles Vulnerability analysis tasks"""

    def __init__(self) -> None:
        # File manager class that is responsible for file operations
        super().__init__()
        self.data = None
        # list of hosts that passed credential check
        self.credentialed_hosts: List[str] = []
        # CSV headers used for analysis
        self.headers: Optional[List[str]] = None  # config.nessus_headers
        # columns to showcase on our final Excel
        self.selected_columns: List[str] = []
        # contains constants to be used across the program
        # Scanner_type
        self.scanner: str = "nessus"
        self.vulnerabilities: List[Dict] = []
        # file_attributes
        self.file_type: str = ""
        self.file_list: List[Dict] = []
        self.starting_index: int = 0
        self.starting_file: str = ""
        self.skip_credential_check = False
        self.debug = True

    @classmethod
    def reset_class_states(cls):
        """Reset the states of the class"""
        cls.data = None
        cls.credentialed_hosts = []
        cls.headers = None
        cls.selected_columns = []
        cls.scanner = "nessus"
        cls.vulnerabilities = []
        cls.file_type = ""
        cls.file_list = []
        cls.starting_index = 0
        cls.starting_file = ""

    def set_scanner(self, scanner: str):
        """
        Set the scanner type and update related configurations

        Args:
            scanner: Type of scanner ('nessus' or 'rapid')
        """
        self.scanner = scanner
        # Update Inherited class attribute
        super().update_scanner(scanner)
        # Set headers based on a scanner type
        self.headers = self.NESSUS_HEADERS if scanner == "nessus" else self.RAPID7_HEADERS

    def set_file_type(self, file_type: str):
        self.file_type = file_type

    def format_input_file(self) -> list:
        """
        Formats the input data based on scanner type and filtering criteria

        Returns:
            Formatted DataFrame with filtered vulnerabilities
        """
        if self.data is None or self.headers is None:
            raise ValueError("Data or headers not set")

        if self.scanner == "nessus":
            return self.format_nessus_data()
        elif self.scanner == "rapid":
            return self.format_rapid7_data()
        else:
            raise ValueError(f"Unsupported scanner: {self.scanner}")

    def _log_perfomance_metrics(self, total_rows: int, filtered_rows: int, unique_hosts: int):
        """Log performance metrics for debugging"""
        self.print_debug_message(
            f"\n Performance Metrics:"
            f"\nTotal Rows: {total_rows}"
            f"\nFiltered Rows: {filtered_rows}"
            f"\nReduction: {((total_rows - filtered_rows)/total_rows)*100:.2f}%"
            f"\nUnique Hosts: {unique_hosts}"
        )

    def format_nessus_data(self):
        """
        Formats Nessus data based on credentialed hosts and selected columns

        Returns:
            Formatted DataFrame with filtered vulnerabilities
        """
        if self.data is None or self.headers is None:
            raise ValueError("Data or headers not set")

        # 1. Create a credential check mask
        if not self.skip_credential_check:
            cred_mask = (self.data["Plugin Output"].notna() &
                         self.data["Plugin Output"].str.contains(
                "Credentialed checks : yes", na=False, regex=False)
            )
            # Get credentialed hosts
            self.credentialed_hosts = self.data.loc[cred_mask, "Host"].unique(
            ).tolist()
            # Filter for those hosts
            host_mask = self.data["Host"].isin(self.credentialed_hosts)
        else:
            # if skip_cred_check, all hosts become valid
            host_mask = self.data["Host"].notna()
            self.credentialed_hosts = self.data.loc[host_mask, "Host"].unique(
            ).tolist()

        # 2. Create Risk filter mask
        risk_mask = (
            self.data["Risk"].notna() &
            ~self.data["Risk"].isin(["Low", "PASSED"])
        )
        # 3. Combine masks and apply filter
        final_mask = host_mask & risk_mask
        formatted_vulnerabilities = (self.data.loc[final_mask, self.headers]
                                     .sort_values(by="Risk")
                                     .reset_index(drop=True))

        total = len(self.credentialed_hosts)
        _msg = (" Credentialed " if not self.skip_credential_check else " Scanned : ")

        self.print_success_message(
            message=f"Total {_msg} Hosts: [{total}]",
            extras=f"{self.credentialed_hosts}"
        )
        if self.debug:
            self._log_perfomance_metrics(
                total_rows=len(self.data),
                filtered_rows=len(formatted_vulnerabilities),
                unique_hosts=len(self.credentialed_hosts)
            )

        # Sort Vulnerabilities using Risk
        return formatted_vulnerabilities

    def format_rapid7_data(self):
        """
        Formats Rapid data based on selected columns

        Returns:
            Formatted DataFrame with filtered vulnerabilities
        """
        # print("TODO: Filter further i.e Credentialed Hosts")
        return self.data[self.headers[:-1]]

    def get_missing_columns(self, dataframe, filename):
        # compare the headers from our defined headers and provided dataframe

        missing_columns = list(set(self.headers) - set(dataframe.columns))
        if missing_columns:
            raise KeyError(
                f"The following columns are missing from the "
                f"{filename} : {missing_columns}"
            )

    def set_scan_attributes(self, attributes: tuple) -> None:
        """
        :param :([list_of_files], index_of_selected_file)
        sets file_list ⇒ list of scanned_files [CSV|XLSX]
             start_index == 0 ⇒ indexes of selected file
             starting_file ⇒ starting file name
        """
        self.file_list = attributes[0]
        self.starting_index = attributes[1]
        self.starting_file = self.file_list[self.starting_index]["full_path"]

    def analyze_scan_files(self, domain: str, csv_data: tuple) -> list | None:
        """
        Analyzes vulnerability scan files and returns formatted results

        Args:
            domain: The assessment domain (e.g., 'internal', 'external')
            csv_data: Tuple containing file information

        Returns:
            Formatted DataFrame of vulnerabilities or None if processing fails
        """

        try:
            # update our storage path
            self.update_output_directory(domain)
            self.set_scan_attributes(csv_data)

            # Process initial file is empty
            original_file = self._process_initial_file()

            if original_file is None:
                return None

            # Process remaining files in parallel
            with ThreadPoolExecutor(max_workers=4)as executor:
                process_func = partial(self._process_files)
                remaining_files = [
                    f for f in self.file_list if f["full_path"] != self.starting_file]
                results = list(executor.map(process_func, remaining_files))

            valid_results = [df for df in results if df is not None]
            if not valid_results:
                return original_file

            all_vulnerabilities = pd.concat(
                [original_file]+valid_results, ignore_index=True)

            self.data = all_vulnerabilities
            return self.format_input_file()

        except Exception as error:
            self.print_error_message(
                message="Error analyzing scan files", exception_error=str(error))
            return None

    def _process_initial_file(self) -> pd.DataFrame | None:
        """Process the initial file and handle errors

        Returns:
            DataFrame from an initial file or None if processing fails
        """
        try:
            filename = self.file_list[self.starting_index]["filename"]

            file_extension = self.get_file_extension(self.starting_file)

            if file_extension.lower() == "csv":
                original_file = self.read_csv(self.starting_file)
            elif file_extension.lower() in ["xlsx", "xls"]:
                original_file = self.read_excel_file(
                    self.starting_file, header=None)
            else:
                raise ValueError(
                    f"Unsupported file extension: {file_extension}")

            # Validate columns against expected headers
            self.get_missing_columns(original_file, filename)

            return original_file

        except Exception as error:

            self.print_error_message(
                message="Error processing initial file: ",
                exception_error=error
            )
            return None

    def _process_files(self, file_info: dict) -> pd.DataFrame | None:
        """Process the initial file and handle errors

        Returns:
            DataFrame from an initial file or None if processing fails
        """
        try:
            filename = file_info["filename"]
            file_path = file_info["full_path"]
            file_extension = self.get_file_extension(file_path)

            if file_extension.lower() == "csv":
                df = self.read_csv(file_path)
            elif file_extension.lower() in ["xlsx", "xls"]:
                df = self.read_excel_file(file_path, header=None)
            else:
                raise ValueError(
                    f"Unsupported file extension: {file_extension}")

            # Validate columns against expected headers
            self.get_missing_columns(df, filename)

            return df

        except Exception as error:

            self.print_error_message(
                message=f"Error processing {file_info["filename"]} with columns {len(df.columns)}: ",
                exception_error=error
            )
            return None

    def filter_condition(self, filter_string: str):
        """Filter CSV file using the key word supplied
        :param: filter_string ⇒ String to use when filtering
        """

        return self.filter_vulnerabilities(
            self.vulnerabilities, filter_param=filter_string
        )[filter_string]

    def categorize_vulnerabilities(self) -> dict:
        """Function returns a dictionary containing tuple
        {(dataframe_name : dataframe)}
        """
        # Apply filtering dynamically
        categorized_vulnerabilities = {}
        categories = None
        if self.scanner == "nessus":
            categories = self.NESSUS_VULN_CATEGORIES

        elif self.scanner == "rapid":
            categories = self.RAPID7_VULN_CATEGORIES

        for key, value in categories.items():
            result = self.filter_condition(value)

            categorized_vulnerabilities[key] = self.vulnerabilities[result]

        return categorized_vulnerabilities

    def _apply_filter(self):
        """Filters based on selected vulnerability scanner"""
        global combined_filter
        filter_strings = ([*self.NESSUS_VULN_CATEGORIES.values()] if
                          self.scanner == "nessus"
                          else [*self.RAPID7_VULN_CATEGORIES.values()])
        masks = [~self.filter_condition(f) for f in filter_strings]
        combined_filter = reduce(lambda x, y: x & y, masks) if masks else True
        # Show remaining data after filtering
        # Start loop from the second value and append results
        # to combined filter

        return self.vulnerabilities[combined_filter]

    def sort_vulnerabilities(self, vulnerabilities, output_file):
        self.vulnerabilities = vulnerabilities

        # get unfiltered vulnerabilities
        unfiltered = self._apply_filter()

        # returns tuple containing key, value pair of dataframe
        # identifier and dataframe

        issues = self.categorize_vulnerabilities()

        # create summary page
        summary_page = self.create_summary_sheet(issues)

        found_vulnerabilities = [
            {"dataframe": issue[1], "sheetname": f"{issue[0]}"}
            for issue in issues.items()
        ]

        # append the first sheet to the list of found vulnerabilities
        found_vulnerabilities.insert(
            0, {"dataframe": summary_page, "sheetname": "Summary"})
        # insert unfiltered vulnerabilities to found vulns
        found_vulnerabilities.insert(
            1, {"dataframe": unfiltered, "sheetname": "Unfiltered"})

        # Write Vulnerabilities to file
        if len(found_vulnerabilities) != 0:
            self.write_to_multiple_sheets(
                found_vulnerabilities,
                output_file,
            )

    def create_summary_sheet(self, issues: dict):
        """Create a summary page for the vulnerability
        :param: issues ⇒ Dictionary containing key, value a pair of dataframe
            :return:
        """
        summary_rows = []

        # Map category names to more readable titles
        vuln_categories = (self.NESSUS_VULN_CATEGORIES if
                           self.scanner == "nessus" else self.RAPID7_VULN_CATEGORIES)

        for condition, dataframe in issues.items():

            if dataframe.empty:
                # print(f"\n{self.FAIL}DEBUG: Dataframe for: {condition} is empty..Skipping{self.ENDC}")
                continue

            # print(f"\nDEBUG: Processing condition: {condition}")
            category_name = next((name for name, cond in vuln_categories.items()
                                  if cond == condition), condition)

            # Group unique vulnerabilities by hosts
            if self.scanner == "nessus":
                hosts = dataframe['Host'].unique()
            else:
                hosts = dataframe['Asset IP Address'].unique()

            # print(f"Debug - Found {len(hosts)} hosts for {category_name}")

            formatted_hosts = '\n'.join(hosts)

            summary_rows.append({
                'S.No': len(summary_rows) + 1,
                'Observation': category_name,
                'Description': '',  # Empty column for manual input
                'Impact': '',  # Empty
                'Risk Rating': '',  # Empty
                'Recommendation': '',  # Empty
                'Affected Hosts': formatted_hosts,
                'Management Response': ''  # Empty column for manual input
            })
        # Create a summary dataframe
        summary_df = self.create_pd_dataframe(
            summary_rows, self.SUMMARY_SHEET_HEADERS)

        risk_order = ['Critical', 'High', 'Medium']
        summary_df['Risk Sort'] = self.sort_dataframe(
            summary_df['Risk Rating'],
            risk_order,
        )
        summary_df = summary_df.sort_values(
            'Risk Sort').drop('Risk Sort', axis=1)
        # print(f"\nDEBUG: - Final dataframe: {summary_df.shape}")
        return summary_df
