from typing import List, Dict, Optional
from handlers import FileHandler, ScreenHandler
from utils import Config, FilterVulnerabilities, CustomDecorators
from functools import reduce
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
import dask.dataframe as dd
import re


class VulnerabilityAnalysis(
    FileHandler,
    Config,
    FilterVulnerabilities,
    ScreenHandler
):
    """Class that handles Vulnerability analysis tasks"""

    def __init__(self) -> None:
        # File manager class that is responsible for file operations
        super().__init__()
        self.data = None
        # list of hosts that passed credential check
        self.credentialed_hosts: List[str] = []
        # CSV headers used for analysis
        self.headers: Optional[List[str]] = None  # config.nessus_headers
        # columns to showcase on our final Excel
        self.selected_columns: List[str] = []
        # contains constants to be used across the program
        # Scanner_type
        self.scanner: str = "nessus"
        self.vulnerabilities: List[Dict] = []
        # file_attributes
        self.file_type: str = ""
        self.file_list: List[Dict] = []
        self.starting_index: int = 0
        self.starting_file: str = ""
        self.skip_credential_check = False
        self.debug = False
        self.timer_enabled = True
        

    @classmethod
    def reset_class_states(cls):
        """Reset the states of the class"""
        cls.data = None
        cls.credentialed_hosts = []
        cls.headers = None
        cls.selected_columns = []
        cls.scanner = "nessus"
        cls.vulnerabilities = []
        cls.file_type = ""
        cls.file_list = []
        cls.starting_index = 0
        cls.starting_file = ""

    def set_scanner(self, scanner: str):
        """Set the scanner type and update related configurations
        Args:
            scanner (str): Type of scanner [Nessus | Rapid7]
        """
        self.scanner = scanner
        # Update Inherited class attribute
        super().update_scanner(scanner)
        # Set headers based on a scanner type
        self.headers = self.NESSUS_HEADERS if scanner == "nessus" else self.RAPID7_HEADERS

    def set_file_type(self, file_type: str):
        self.file_type = file_type

    def format_input_file(self) -> list:
        """
        Formats the input data based on scanner type and filtering criteria

        Returns:
            Formatted DataFrame with filtered vulnerabilities
        """
        if self.data is None or self.headers is None:
            raise ValueError("Data or headers not set")

        if self.scanner == "nessus":
            return self.format_nessus_data()
        elif self.scanner == "rapid":
            return self.format_rapid7_data()
        else:
            raise ValueError(f"Unsupported scanner: {self.scanner}")

    def _log_perfomance_metrics(self, total_rows: int, filtered_rows: int, unique_hosts: int):
        """Log performance metrics for debugging"""
        self.print_debug_message(
            f"\n Performance Metrics:"
            f"\nTotal Rows: {total_rows}"
            f"\nFiltered Rows: {filtered_rows}"
            f"\nReduction: {((total_rows - filtered_rows)/total_rows)*100:.2f}%"
            f"\nUnique Hosts: {unique_hosts}"
        )

    def format_nessus_data(self):
        """
        Formats Nessus data based on credentialed hosts and selected columns

        Returns:
            Formatted DataFrame with filtered vulnerabilities
        """
        if self.data is None or self.headers is None:
            raise ValueError("Data or headers not set")

        # 1. Create a credential check mask
        if not self.skip_credential_check:
            cred_pattern = "Credentialed checks : yes"
            cred_mask = self.data["Plugin Output"].str.contains(
                cred_pattern, case=True, na=False)
            if cred_mask.sum() == 0:
                self.print_warning_message("No Credentialed Hosts found")
                host_mask = self.data["Plugin Output"].notna()
            else:
                self.credentialed_hosts = self.data.loc[cred_mask, "Host"].unique(
                ).tolist()
                host_mask = self.data["Host"].isin(self.credentialed_hosts)
        else:
            host_mask = self.data["Plugin Output"].notna()
            self.credentialed_hosts = self.data.loc[host_mask, "Host"].unique(
            ).tolist()

        # Single-pass filter with host and risk criteria
        final_mask = (
            host_mask &
            self.data["Risk"].notna() &
            ~self.data["Risk"].isin(["Low", "PASSED"])
        )
        formatted_vulnerabilities = (
            self.data.loc[final_mask, self.headers]
            .sort_values(by="Risk", na_position="last")
            .reset_index(drop=True))

        total = len(self.credentialed_hosts)
        _msg = (" Credentialed " if not self.skip_credential_check else " Scanned : ")

        self.print_success_message(
            message=f"Total {_msg} Hosts: [{total}]",
            extras=f"{self.credentialed_hosts}"
        )
        if self.debug:
            self._log_perfomance_metrics(
                total_rows=len(self.data),
                filtered_rows=len(formatted_vulnerabilities),
                unique_hosts=len(self.credentialed_hosts)
            )
        if formatted_vulnerabilities.empty:
            self.print_error_message(
                "No vulnerabilities found after filtering")
            return None
        # Sort Vulnerabilities using Risk
        return formatted_vulnerabilities

    def format_rapid7_data(self):
        """
        Formats Rapid data based on selected columns

        Returns:
            Formatted DataFrame with filtered vulnerabilities
        """
        # print("TODO: Filter further i.e Credentialed Hosts")
        return self.data[self.headers[:-1]]

    def get_missing_columns(self, dataframe, filename):
        # compare the headers from our defined headers and provided dataframe

        missing_columns = list(set(self.headers) - set(dataframe.columns))
        if missing_columns:
            raise KeyError(
                f"The following columns are missing from the "
                f"{filename} : {missing_columns}"
            )

    def set_scan_attributes(self, attributes: tuple) -> None:
        """
        :param :([list_of_files], index_of_selected_file)
        sets file_list ⇒ list of scanned_files [CSV|XLSX]
             start_index == 0 ⇒ indexes of selected file
             starting_file ⇒ starting file name
        """
        self.file_list = attributes[0]
        self.starting_index = attributes[1]
        self.starting_file = self.file_list[self.starting_index]["full_path"]

    def read_csv_file(self, file):
        try:
            df = pd.read_csv(
                file,
                quotechar='"',
                encoding="utf-8",
                dtype="string")
            if self.debug:
                self.print_debug_message(
                    f"Read {len(df)} rows from {file}")
            return df

        except pd.errors.ParserError as error:  # Incase of malformed CSV
            self.print_error_message(
                message="Error reading CSV file",
                exception_error=str(error)
            )
            # Exctract ruw number from error message
            error_message = str(error)
            row_match = re.search(r"row (\d+)", error_message)
            bad_row = int(row_match.group(1)) if row_match else 0

            if bad_row:
                if self.debug:
                    self.print_debug_message(
                        f"Bad row found at {bad_row} in {file}")
                # Partial read up to bad row
                try:
                    df = pd.read_csv(
                        file,
                        quotechar='"',
                        nrows=bad_row - 1,
                        encoding="utf-8",
                        dtype=str
                    )
                    if self.debug:
                        self.print_debug_message(
                            f"Read {len(df)} rows from {file}")
                    # Log problematic row
                    with open(file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                        if len(lines) > bad_row:
                            self.print_debug_message(
                                f"Row {bad_row}: {lines[bad_row-1]}")

                    return df
                except Exception as error2:
                    # Partial read up to bad row
                    self.print_error_message(
                        message=f"Partial read failed for file {file}",
                        exception_error=str(error2)
                    )
                    return None
            try:
                # Fallback to Python engine if row not found
                df = pd.read_csv(
                    file,
                    quotechar='"',
                    escapechar='\\',
                    encoding="utf-8",
                    engine="python",
                    on_bad_lines='warn'
                )
                if self.debug:
                    self.print_debug_message(
                        f"Read {len(df)} rows from {file}")
                return df
            except Exception as error3:
                self.print_error_message(
                    message=f"Python enfine failed for file {file}",
                    exception_error=str(error3)
                )
                return None
        except Exception as error:
            self.print_error_message(
                f"Unexpected error reading file {file}", exception_error=str(error))
            return None

    def analyze_scan_files(self, domain: str, csv_data: tuple) -> list | None:
        """
        Analyzes vulnerability scan files and returns formatted results
        Gets list of files and groups the files by their extension for parallel processing

        Args:
            domain: The assessment domain (e.g., 'internal', 'external')
            csv_data: Tuple containing file information

        Returns:
            Formatted DataFrame of vulnerabilities or None if processing fails
        """

        try:
            # update our storage path
            # self.print_debug_message("Setting up analysis environment")
            self.update_output_directory(domain)
            self.set_scan_attributes(csv_data)

            # Validate and group files by extensions
            file_paths = [f["full_path"] for f in self.file_list]
            valid_extensions = {"csv", "xlsx", "xls"}
            ext_groups = {"csv": [], "excel": []}

            for file_path in file_paths:
                ext = self.get_file_extension(file_path)
                if ext not in valid_extensions:
                    raise ValueError(
                        f"Unsupported file extension '{ext}' in file: {file_path}")
                # set file_type
                if ext == "csv":
                    ext_groups["csv"].append(file_path)
                elif ext in ["xlsx", "xls"]:
                    ext_groups["excel"].append(file_path)

            # Process files based on extension groups
            dataframes = []

            # Handles CSV files with Dask
            if ext_groups["csv"]:

                try:
                    csv_ddf = dd.read_csv(
                        ext_groups["csv"], assume_missing=True, dtype="string")
                    dataframes.append(csv_ddf.compute())

                except Exception:
                    # Fallback to reading CSV files with ThreadPoolExecutor
                    with ThreadPoolExecutor(max_workers=4) as executor:
                        csv_dfs = list(executor.map(
                            self.read_csv_file, ext_groups["csv"]))
                        dataframes.extend(csv_dfs)

            # Handle Excel files with ThreadPoolExecutor
            if ext_groups["excel"]:
                with ThreadPoolExecutor(max_workers=4) as executor:
                    excel_dfs = list(executor.map(lambda f: self.read_excel_file(
                        f, header=None), ext_groups["excel"]))
                    if excel_dfs:
                        excel_combined = pd.concat(
                            excel_dfs, ignore_index=True)
                        dataframes.append(excel_combined)

            # Combine all DataFrames
            if not dataframes:
                raise ValueError("No valid files processed")

            self.data = pd.concat(dataframes, ignore_index=True)if len(
                dataframes) > 1 else dataframes[0]

            self.get_missing_columns(self.data, "combined_files")
            formatted_vulnerabilities = self.format_input_file()
            if formatted_vulnerabilities is None or formatted_vulnerabilities.empty:
                self.print_error_message("No vulnerabilities found to analyze")
                return None
            return formatted_vulnerabilities

        except Exception as error:
            self.print_error_message(
                message="Error analyzing scan files", exception_error=str(error))
            return None

    def _process_initial_file(self) -> pd.DataFrame | None:
        """Process the initial file and handle errors

        Returns:
            DataFrame from an initial file or None if processing fails
        """
        try:
            filename = self.file_list[self.starting_index]["filename"]

            file_extension = self.get_file_extension(self.starting_file)

            if file_extension.lower() == "csv":
                original_file = self.read_csv(self.starting_file)
            elif file_extension.lower() in ["xlsx", "xls"]:
                original_file = self.read_excel_file(
                    self.starting_file, header=None)
            else:
                raise ValueError(
                    f"Unsupported file extension: {file_extension}")

            # Validate columns against expected headers
            self.get_missing_columns(original_file, filename)

            return original_file

        except Exception as error:

            self.print_error_message(
                message="Error processing initial file: ",
                exception_error=error
            )
            return None

    def _process_files(self, file_info: dict) -> pd.DataFrame | None:
        """Process the initial file and handle errors

        Returns:
            DataFrame from an initial file or None if processing fails
        """
        try:
            filename = file_info["filename"]
            file_path = file_info["full_path"]
            file_extension = self.get_file_extension(file_path)

            if file_extension.lower() == "csv":
                df = self.read_csv(file_path)
            elif file_extension.lower() in ["xlsx", "xls"]:
                df = self.read_excel_file(file_path, header=None)
            else:
                raise ValueError(
                    f"Unsupported file extension: {file_extension}")

            # Validate columns against expected headers
            self.get_missing_columns(df, filename)

            return df

        except Exception as error:

            self.print_error_message(
                message=f"Error processing {file_info["filename"]} with columns {len(df.columns)}: ",
                exception_error=error
            )
            return None

    def filter_condition(self, filter_string: str):
        """Filter CSV file using the key word supplied
        :param: filter_string ⇒ String to use when filtering
        """
        return self.filter_vulnerabilities(
            self.vulnerabilities, filter_param=filter_string
        )[filter_string]

    def categorize_vulnerabilities(self) -> dict:
        """Function returns a dictionary containing tuple
        {(dataframe_name : dataframe)}
        """
        # Apply filtering dynamically
        categorized_vulnerabilities = {}
        categories = (self.NESSUS_VULN_CATEGORIES if self.scanner ==
                      "nessus" else self.RAPID7_VULN_CATEGORIES)

        for key, value in categories.items():

            result = self.filter_condition(value)
            categorized_vulnerabilities[key] = self.vulnerabilities[result]

        return categorized_vulnerabilities

    def _apply_filter(self):
        """Filters based on selected vulnerability scanner"""
        global combined_filter
        filter_strings = ([*self.NESSUS_VULN_CATEGORIES.values()] if
                          self.scanner == "nessus"
                          else [*self.RAPID7_VULN_CATEGORIES.values()])

        masks = [~self.filter_condition(f) for f in filter_strings]
        combined_filter = reduce(lambda x, y: x & y, masks) if masks else True
        return self.vulnerabilities[combined_filter]

    
    def sort_vulnerabilities(self, vulnerabilities: pd.DataFrame, output_file: str):
        """Sort vulnerabilities based on scanner type and filters
        :param: vulnerabilities ⇒ DataFrame containing vulnerabilities
        :param: output_file ⇒ Filename to save the sorted vulnerabilities
        """
        try:
            if vulnerabilities is None or vulnerabilities.empty:
                raise ValueError("No vulnerabilities provided for sorting")
            if self.debug:
                self.print_debug_message(
                    f"Sorting {len(vulnerabilities)} vulnerabilities..")

            self.vulnerabilities = vulnerabilities
            # get unfiltered vulnerabilities
            unfiltered = self._apply_filter()
            if unfiltered is None:
                raise ValueError("Error applying filters...")
            if self.debug:
                self.print_debug_message(
                    f"Found {len(unfiltered)} unfiltered vulnerabilities")

            if self.debug:
                self.print_debug_message("Categorizing vulnerabilities ...")

            issues = self.categorize_vulnerabilities()
            if not issues:
                raise ValueError("No issues found after categorization")

            if self.debug:
                issue_count = 0
                for _, value in issues.items():
                    # increment count if len of value not 0
                    if len(value) != 0:
                        issue_count += 1

                self.print_debug_message(
                    f"\nCategorized issues: {issue_count}")
            # create summary page
            summary_page = self.create_summary_sheet(issues)

            found_vulnerabilities = [{"dataframe": issue[1], "sheetname": f"{issue[0]}"}
                                     for issue in issues.items()]

            # append the first sheet to the list of found vulnerabilities
            found_vulnerabilities.insert(
                0, {"dataframe": summary_page, "sheetname": "Summary"})
            # insert unfiltered vulnerabilities to found vulns
            found_vulnerabilities.insert(
                1, {"dataframe": unfiltered, "sheetname": "Unfiltered"})

            # Write Vulnerabilities to file
            if len(found_vulnerabilities) != 0:
                self.write_to_multiple_sheets(
                    found_vulnerabilities,
                    output_file,
                )
            # Print Time Taken
            if self.timer_enabled:
                self.print_total_time(
                    f"Total time taken for {self.scanner.title()} Vulnerability analysis:")

            return True
        except Exception as error:
            self.print_error_message(
                message="Error sorting vulnerabilities", exception_error=str(error)
            )
            return False
        finally:
            # Reset after run

            self.reset_total_time()

    def create_summary_sheet(self, issues: dict):
        """Create a summary page for the vulnerability
        :param: issues ⇒ Dictionary containing key, value a pair of dataframe
            :return:
        """
        summary_rows = []

        # Map category names to more readable titles
        vuln_categories = (self.NESSUS_VULN_CATEGORIES if
                           self.scanner == "nessus" else self.RAPID7_VULN_CATEGORIES)

        for condition, dataframe in issues.items():

            if dataframe.empty:
                continue

            if self.debug:
                self.print_debug_message(f" Processing condition: {condition}")

            category_name = next((name for name, cond in vuln_categories.items()
                                  if cond == condition), condition)

            # Group unique vulnerabilities by hosts
            if self.scanner == "nessus":
                hosts = dataframe['Host'].unique()
            else:
                hosts = dataframe['Asset IP Address'].unique()

            if self.debug:
                self.print_debug_message(
                    f"Debug - Found {len(hosts)} hosts for {category_name}")

            formatted_hosts = '\n'.join(hosts)

            summary_rows.append({
                'S.No': len(summary_rows) + 1,
                'Observation': category_name,
                'Description': '',  # Empty column for manual input
                'Impact': '',  # Empty
                'Risk Rating': '',  # Empty
                'Recommendation': '',  # Empty
                'Affected Hosts': formatted_hosts,
                'Management Response': ''  # Empty column for manual input
            })
        # Create a summary dataframe
        summary_df = self.create_pd_dataframe(
            summary_rows, self.SUMMARY_SHEET_HEADERS)

        risk_order = ['Critical', 'High', 'Medium']
        summary_df['Risk Sort'] = self.sort_dataframe(
            summary_df['Risk Rating'],
            risk_order,
        )
        summary_df = summary_df.sort_values(
            'Risk Sort').drop('Risk Sort', axis=1)

        return summary_df
